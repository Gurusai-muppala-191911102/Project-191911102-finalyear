# -*- coding: utf-8 -*-
"""saiprojectpaper1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bQLB3ypdtpwYJl_TgvH1UokEAq9H35zY
"""

from google.colab import drive
drive.mount('/content/gdrive')

pip install rake-nltk

pip install nltk

import nltk
nltk.download('stopwords')

nltk.download('punkt')

import nltk.data
from nltk import sent_tokenize, word_tokenize, PorterStemmer

file1 = open('/content/gdrive/MyDrive/projectfolder/CharlestonFeb28_2020.txt','r')
data1 = file1.read()
sentences = sent_tokenize(data1)
split_words=data1.split(" ")
print(len(sentences))
print(len(data1.split(" ")))
for i in split_words:
  print(i)

manual_file = open("/content/gdrive/MyDrive/projectfolder/manual_keywordsry.txt",'r')
data_manual = manual_file.read()
manual_phrases = data_manual.split("\n")
print(len(manual_phrases))
for i in manual_phrases:
  print(i)

from rake_nltk import Rake
import random
import time
r = Rake()

file2 = open("/content/gdrive/MyDrive/projectfolder/output.txt",'w')

def rake_measures(y,manual_keywords):
  common = set(y) & set(manual_keywords)
  TP= len(common)
  FP  = len(y)-TP
  FN  = len(manual_keywords)-TP
  precision = (TP)/(TP+FP)
  recall =  (TP)/len(manual_keywords)
  f_measure = (2*precision*recall)/(precision+recall)
  miss_rate = FN/(TP+FN)
  '''
  Accuracy = (TP+TN)/ TOTAL
  error_rate = (FP+FN)/ TOTAL
  specificity = (TN)/(FP+TN)
  '''
  print("precision=",precision," recall=",recall," f-measure=",f_measure)
  #print("TP=",TP,"FP",FP,"FN=",FN)

def rake(data):
  rake_phrases={}
  rake_accuracy = {}
  rake_error_rate = {}
  count = 0
  
  for i in [5,10,15,20]:
    start_time_rake = time.time()
    r.extract_keywords_from_text(data)
    t = r.get_ranked_phrases()[:i]
    y = r.get_ranked_phrases_with_scores()[:i]
    end_time_rake = time.time()
    #manual_keywords=random.sample(y, int((1/3)*len(y)))
    time_yake = end_time_rake-start_time_rake
    
    q = []
    for j in y:
      print(j[1])
      if j[1] in manual_phrases:
        q.append(j[1])
    rake_phrases[i]=len(set(q))
    accuracy = float((len(set(q))/i)*100)
    rake_accuracy[i]= accuracy
    rake_error_rate[i] = (100-accuracy)
    print("the RAKE measures for ",i," rake extracted keywords :")
    rake_measures(t,manual_phrases)
    print()

  return rake_phrases,rake_accuracy
  #print(manual_keywords)
  

rake_sol1,rake_sol2 = rake(data1)

all_words=[]
def rake_write_data(data):
  r.extract_keywords_from_text(data)
  t = r.get_ranked_phrases()
  t=set(t)
  for i in t:
    words = i.split(" ")
    for j in words:
      all_words.append(j)
rake_write_data(data1)
all_words=set(all_words)

for i in all_words:
  if i.isalpha():
    file2.write(i+" ")

pip install yake

import yake
file3 = open("/content/gdrive/MyDrive/projectfolder/super1.txt",'w')

all_yake_words = []
def yake_algo(data):
  yake_phrases = {}
  yake_accuracy = {}
  count= 0
  kw_extractor = yake.KeywordExtractor()
  keywords = kw_extractor.extract_keywords(data)
  for i in keywords:
    words = i[0].split(" ")
    for j in words:
      all_yake_words.append(j)

  for i in [5,10,15,20]:
    count = 0
    start_time_rake = time.time()
    keywords = kw_extractor.extract_keywords(data)[:i]
    end_time_rake = time.time()
    time_yake = end_time_rake-start_time_rake

    for j in keywords:
      print(j[0])
      if j[0] in manual_phrases:
        count+=1
    accuracy = float((count/i)*100)
    yake_phrases[i]=count
    yake_accuracy[i]= accuracy

  return yake_phrases,yake_accuracy
yake_sol1,yake_sol2 = yake_algo(data1)

all_yake_words = set(all_yake_words)
for i in all_yake_words:
  if i.isalpha():
    file3.write(i+" ")

from matplotlib import pyplot as plt  
plt.bar(list(rake_sol1.keys()),list(rake_sol1.values()),color='green')  
plt.bar(list(yake_sol1.keys()),list(yake_sol1.values()),color='red')
plt.xlabel("No. of keywords extracted by RAKE")
plt.ylabel("No. of keywords matched with manual keywords")
plt.title('RAKE vs YAKE') 
plt.show();

from matplotlib import pyplot as plt  
plt.plot(list(rake_sol1.keys()),list(rake_sol1.values()),color='green',label="RAKE",marker='+')  
plt.plot(list(yake_sol1.keys()),list(yake_sol1.values()),color='red',label="YAKE",marker='+')
plt.xlabel("No. of keywords extracted by RAKE and YAKE")
plt.ylabel("No. of keywords matched with manual keywords")
plt.title('RAKE vs YAKE') 
plt.legend(loc='upper left')
plt.show();

import pandas as pd
myList=[list(rake_sol1.values()),list(yake_sol1.values())]
myDf=pd.DataFrame(myList,columns=list(rake_sol1.keys()))
myDf.index=["no. of RAKE keywords matched","no. of YAKE keywords matched"]
newDf=myDf.T
print(newDf)

from matplotlib import pyplot as plt  
plt.plot(list(rake_sol2.keys()),list(rake_sol2.values()),color='green',label="RAKE",marker='+')  
plt.plot(list(yake_sol2.keys()),list(yake_sol2.values()),color='red',label="YAKE",marker='+')
plt.xlabel("No. of keywords extracted by RAKE and YAKE")
plt.ylabel("Accuracy")
plt.title('RAKE vs YAKE ACCURACY') 
plt.legend(loc='upper left')
plt.show();

pip install spacy

pip install <model_s3_url>

pip install en_core_web_lg

import spacy
nlp = spacy.load('en_core_web_sm')
doc1 = nlp(u'/content/gdrive/MyDrive/projectfolder/output.txt')
doc2 = nlp(u'/content/gdrive/MyDrive/projectfolder/CharlestonFeb28_2020.txt')
n  = (nlp(doc1).similarity(nlp(doc2)))
print(n*100)

sum1_rake=sum(list(rake_sol1.values()))
sum2_rake=sum(rake_sol1.keys())
print("accuracy of RAKE:")
print((sum1_rake/sum2_rake)*100)
sum1_yake=sum(list(yake_sol1.values()))
sum2_yake=sum(yake_sol1.keys())
print("accuracy of YAKE:")
print((sum1_yake/sum2_yake)*100)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk import tokenize,word_tokenize
def main_c(txt):
        noOfSentences=15
        stop_words=set(stopwords.words('english'))
        sentences = txt.split(".")
        #print(sentences)

        #user wants to reduce the content to "no_sentences" lines.                    
        def summarize(text,no_sentences):
                word_weights={}
                for word in word_tokenize(text):
                        word=word.lower()
                        if len(word) >1 and word not in stop_words:
                            if word in word_weights.keys():
                                word_weights[word]+=1
                            else:
                                word_weights[word]=1
                sentence_weights={}
                for sent in tokenize.sent_tokenize(text):
                        sentence_weights[sent]=0
                        for word in word_tokenize(sent):
                            word=word.lower()
                            if word in word_weights.keys():
                                sentence_weights[sent]+=word_weights[word]
                highest_weights = sorted(sentence_weights.values())[-no_sentences:] # draw the graphs for the highest words and highest weighted sentences
                summary=""
                for sentence,strength in sentence_weights.items():
                        if strength in highest_weights:
                            summary+=sentence+" "
                summary = summary.replace("_"," ").strip()
                return summary

        all_reducetext1=summarize(txt,noOfSentences)
        sentences=all_reducetext1.split(".")
        return sentences

R = main_c(data1)

from graphviz import Digraph
g = Digraph('unix', filename='concept-map2',node_attr={'color': 'lightblue2', 'style': 'filled','shape': 'box'})

for i in range(len(R)-1):
  g.edge(R[i],R[i+1])
  g.view()

from google.colab import files
files.download("concept-map2.pdf")
